{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17649e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_path = 'D:/F/Machine Learning/MSML602/Face Data for Homework/ATT'\n",
    "\n",
    "# lists to store images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for subject_id in range(1, 41):\n",
    "    for img_id in range(1, 11):\n",
    "        file_name = f\"{subject_id}_{img_id}.png\"\n",
    "        img_path = os.path.join(data_path, file_name)\n",
    "        \n",
    "        # Check if the file exists\n",
    "        if os.path.exists(img_path):\n",
    "            \n",
    "            # Read the image in grayscale\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is not None:\n",
    "                    images.append(img.flatten())  # Flatten to 1D vector\n",
    "                    labels.append(subject_id - 1) # Label as (subject_id - 1) to start from 0\n",
    "            else:\n",
    "                print(f\"Warning: Couldn't read {img_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: {img_path} does not exist\")\n",
    "\n",
    "# Converting lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2770e877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 10304)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba193de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_with_covariance(data, num_components):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        data (numpy.ndarray): The data matrix of shape (n_samples, n_features).\n",
    "        num_components (int): The number of principal components to keep.\n",
    "\n",
    "    Returns:\n",
    "        reduced_data (numpy.ndarray): Data projected onto the top k principal components.\n",
    "        mean_vector (numpy.ndarray): Mean of the original data.\n",
    "        principal_components (numpy.ndarray): Top k principal components.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the mean vector and center the data\n",
    "    mean_vector = np.mean(data, axis=0)\n",
    "    centered_data = data - mean_vector  # A = X - xÌ„\n",
    "\n",
    "    # Step 2: Compute the covariance matrix (A^T A)\n",
    "    covariance_matrix = centered_data.T @ centered_data / data.shape[0]\n",
    "\n",
    "    # Step 3: Perform eigen decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # Step 4: Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    top_eigenvectors = np.real(eigenvectors[:, sorted_indices[:num_components]])\n",
    "\n",
    "    # Step 5: Project the data onto the top k eigenvectors\n",
    "    reduced_data = centered_data @ top_eigenvectors\n",
    "    return reduced_data, mean_vector, top_eigenvectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26149a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1 - vec2) ** 2))\n",
    "\n",
    "def knn_1nn(test_image, training_images, training_labels):\n",
    "    # Step 1: Calculate distances and store them with labels\n",
    "    distances = [(euclidean_distance(test_image, train_img), training_labels[n])for n, train_img in enumerate(training_images)]\n",
    "    return sorted(distances, key=lambda x: x[0])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318f3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_5_fold_pca(images, labels, num_components):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        images (numpy.ndarray): Image dataset of shape (n_samples, n_features).\n",
    "        labels (numpy.ndarray): Labels corresponding to the images.\n",
    "        num_components (int): Number of principal components for PCA.\n",
    "\n",
    "    Returns:\n",
    "        avg_accuracy (float): Average accuracy across the 5 folds.\n",
    "    \"\"\"\n",
    "    fold_size = len(images) // 5\n",
    "    indices = np.arange(len(images))\n",
    "    np.random.shuffle(indices)\n",
    "    accuracies = []\n",
    "\n",
    "    for fold in range(5):\n",
    "        test_indices = indices[fold * fold_size : (fold + 1) * fold_size]\n",
    "        train_indices = np.setdiff1d(indices, test_indices)\n",
    "\n",
    "        train_images, test_images = images[train_indices], images[test_indices]\n",
    "        train_labels, test_labels = labels[train_indices], labels[test_indices]\n",
    "\n",
    "        # Apply PCA to training images\n",
    "        reduced_train_images, mean_image, top_eigenvectors = pca_with_covariance(\n",
    "            train_images, num_components\n",
    "        )\n",
    "\n",
    "        # Center and project test images\n",
    "        centered_test_images = test_images - mean_image\n",
    "        reduced_test_images = centered_test_images @ top_eigenvectors\n",
    "\n",
    "        # Test each image\n",
    "        correct_predictions = sum(\n",
    "            knn_1nn(test_img, reduced_train_images, train_labels) == test_label\n",
    "            for test_img, test_label in zip(reduced_test_images, test_labels)\n",
    "        )\n",
    "        \n",
    "        # Calculate accuracy for the current fold\n",
    "        fold_accuracy = correct_predictions / len(test_images)\n",
    "        accuracies.append(fold_accuracy)\n",
    "        print(f\"Fold {fold + 1} Accuracy: {fold_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate and return the average accuracy across all folds\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    return avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f5b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 93.75%\n",
      "Fold 2 Accuracy: 97.50%\n",
      "Fold 3 Accuracy: 93.75%\n",
      "Fold 4 Accuracy: 93.75%\n",
      "Fold 5 Accuracy: 97.50%\n",
      "Average Accuracy with PCA (60 components): 95.25%\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "num_components = 60\n",
    "avg_accuracy = cross_validate_5_fold_pca(images, labels, num_components)\n",
    "print(f\"Average Accuracy with PCA (60 components): {avg_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895b6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
