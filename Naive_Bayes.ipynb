{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02c9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "dir= \"D:/F/Machine Learning/MSML602/20_newsgroups\"\n",
    "check= \"Subject\"\n",
    "\n",
    "data=[]\n",
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b71bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name in os.listdir(dir):\n",
    "    folder_path = os.path.join(dir, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            with open(file_path, 'r') as f:\n",
    "                \n",
    "                lines = f.readlines()\n",
    "                subject_index = next((i for i, line in enumerate(lines) if check in line), None)\n",
    "\n",
    "                if subject_index is not None:\n",
    "                    \n",
    "                    content = \"\".join(lines[subject_index + 1:])\n",
    "                    data.append(content)\n",
    "                    labels.append(folder_name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da23ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "class_data = defaultdict(list)\n",
    "\n",
    "for content, label in zip(data, labels):\n",
    "    class_data[label].append(content)\n",
    "    \n",
    "for label, contents in class_data.items():\n",
    "    random.shuffle(contents)\n",
    "    split_index = int(len(contents)) // 2\n",
    "\n",
    "    train_data.extend(contents[:split_index])\n",
    "    train_labels.extend([label] * split_index)\n",
    "    test_data.extend(contents[split_index:])\n",
    "    test_labels.extend([label] * (len(contents) - split_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5e3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data)\n",
    "\n",
    "#1D NumPy array containing frequency of each word\n",
    "word_counts = X_train.sum(axis=0).A1\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "word_freq = list(zip(vocab, word_counts))\n",
    "word_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 200 most frequent words\n",
    "top_200_words = {word for word, _ in word_freq[:200]}\n",
    "top_200_words_list=list(top_200_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3137bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vectorizer = TfidfVectorizer(stop_words=top_200_words_list)\n",
    "X_train = filtered_vectorizer.fit_transform(train_data)\n",
    "X_test = filtered_vectorizer.transform(test_data)\n",
    "\n",
    "train_data_words = vectorizer.inverse_transform(X_train)\n",
    "test_data_words = vectorizer.inverse_transform(X_test)\n",
    "\n",
    "train_data_words = [list(doc) for doc in train_data_words]\n",
    "test_data_words = [list(doc) for doc in test_data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e9e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(D, C):\n",
    "    \n",
    "    N_doc = len(D)  \n",
    "    classes = set(C) \n",
    "\n",
    "    logprior = {}\n",
    "    loglikelihood = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    bigdoc = defaultdict(list)  \n",
    "    \n",
    "    for doc, label in zip(D, C):\n",
    "        bigdoc[label].extend(doc)\n",
    "\n",
    "    V = set(word for words in bigdoc.values() for word in words)\n",
    "\n",
    "    for c in classes:\n",
    "        N_c = sum(1 for label in C if label == c)  # Documents in class c\n",
    "        logprior[c] = np.log(N_c / N_doc)\n",
    "\n",
    "    for c in classes:\n",
    "        word_counts = Counter(bigdoc[c])  # Count words in class C\n",
    "        total_count = sum(word_counts.values())  # Total words in class c\n",
    "\n",
    "        # Calculate log likelihood with Laplace smoothing\n",
    "        for w in V:\n",
    "            loglikelihood[w][c] = np.log((word_counts[w] + 1) / (total_count + len(V)))\n",
    "\n",
    "    return logprior, loglikelihood, V\n",
    "\n",
    "def test_naive_bayes(test_data, logprior, loglikelihood, classes, V):\n",
    "    predictions=[]\n",
    "    \n",
    "    for doc in test_data:\n",
    "        sum_scores = defaultdict(float)  # Store scores for each class\n",
    "\n",
    "        for c in classes:\n",
    "            sum_scores[c] = logprior[c]\n",
    "\n",
    "            for word in doc:\n",
    "                if word in V: \n",
    "                    sum_scores[c] += loglikelihood[word][c]\n",
    "\n",
    "        predictions.append(max(sum_scores, key=sum_scores.get))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c0002e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Prediction Accuracy: 84.13\n"
     ]
    }
   ],
   "source": [
    "classes=set(train_labels)\n",
    "logprior, loglikelihood, vocab = train_naive_bayes(train_data_words, train_labels)\n",
    "\n",
    "y_pred = test_naive_bayes(test_data_words, logprior, loglikelihood, classes, vocab)\n",
    "\n",
    "correct_predictions = sum(1 for true, pred in zip(test_labels, y_pred) if true == pred)\n",
    "accuracy = (correct_predictions / len(test_labels))*100\n",
    "\n",
    "print(f\"Average Prediction Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56259b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
